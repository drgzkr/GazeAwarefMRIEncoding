# Gaze-Aware fMRI Encoding with CNN Features

Code for reproducing the results in:

> **[Neural network-based encoding in free-viewing fMRI with gaze-aware models](https://www.biorxiv.org/content/10.1101/2025.03.14.643260v3.full)**
> Dora Gözükara, Nasir Ahmad, Katja Seeliger, Djamari Oetringer, Linda Geerligs
> *bioRxiv*, 2026.

---

## Contents

- [Setup](#setup)
- [Data](#data)
- [Pipeline](#pipeline)
- [Analyses and Figures](#analyses-and-figures)
- [Dependencies](#dependencies)
- [Citation](#citation)

---

## Setup

Clone the repository and set your data root directory in `config.py`:

```python
# config.py
BASE_DIR = '/path/to/your/data'
```

All other paths are derived from `BASE_DIR` automatically. Running any script will create all required subdirectories if they do not already exist. The expected directory structure is:

```
BASE_DIR/
├── raw/
│   ├── remodnav/          # input .tsv eye-tracking files
│   ├── frames/            # movie frames (subdirs: Run1/ ... Run8/)
│   ├── fmri/              # preprocessed fMRI data
│   ├── prf/               # population receptive field estimates
│   └── atlas/             # AAL mask and reference functional NIfTI
└── derivatives/
    ├── fixations/         # output of step 1
    ├── vgg_features/
    │   ├── by_run/        # output of step 2
    │   ├── by_sub/        # output of step 3
    │   └── hyperlayers/   # output of step 4
    ├── results/           # output of step 5
    ├── weights_and_gaze/  # precomputed weights and gaze pkl files (for analysis.py)
    └── figures/           # output of analysis.py
```

---

## Data

The following data must be obtained and placed under `BASE_DIR/raw/` before running the pipeline. All data are from the [StudyForrest project](https://www.studyforrest.org/).

### Eye-tracking data — `raw/remodnav/`

Remodnav-classified eye-tracking event files, one per subject per run.

- **Format:** `{sub}-run{run}.tsv`
- **Source:** Pre-classified files are available from the [StudyForrest eye-tracking release](https://github.com/psychoinformatics-de/studyforrest-data-eyemovementlabels). Alternatively, raw physio recordings can be classified yourself using `run_remodnav.sh` (see Step 0 below).
- **Processing:** Files are the output of [Remodnav](https://github.com/psychoinformatics-de/remodnav) applied to the raw EyeLink recordings (1000 Hz). The pixel pitch parameter for the StudyForrest display is `0.03299221912188374`.

### Movie stimulus frames — `raw/frames/`

Individual frames extracted from the Forrest Gump movie stimulus at native resolution (1280×544), organised into one subdirectory per run.

- **Format:** `Run{run}/frame{N}.jpg`
- **Source:** Frames must be extracted from the stimulus file provided with the StudyForrest dataset

### fMRI data — `raw/fmri/`

Preprocessed, denoised, and z-scored fMRI timeseries for movie-viewing runs, concatenated across all 8 runs per subject.

- **Format:** `normalized_masked_concat_movie_{sub}.npy` — shape `(N_voxels, 3599)`
- **Source:** Pre-processing follows [Liu et al. (2019)](https://doi.org/10.1038/s41597-019-0303-3), with additional co-registration to MNI space and run-wise z-scoring applied by the authors of this paper
- **Mask:** Voxels are restricted to visual regions of interest defined by the AAL atlas, resulting in 19,629 voxels

### Population receptive field estimates — `raw/prf/`

Voxel-wise pRF estimates (x, y coordinates in visual degrees) derived from the retinotopic mapping runs included in the StudyForrest dataset. Only required for `precision_gaze_and_prf.py`.

- **Format:** `no_threshold_prf_estimates_array_cover10_{sub}.npy`
- **Source:** Estimated from the StudyForrest retinotopic mapping data using the coarse-to-fine method of [Dumoulin & Wandell (2008)](https://doi.org/10.1016/j.neuroimage.2007.09.034)

### Atlas files — `raw/atlas/`

Two files are needed by `analysis.py` to project results into MNI space and onto the cortical surface.

- **AAL mask:** `aal_mask_downsampled_with_func_mni_3iso_nan_bb.nii` — the AAL visual ROI mask resampled to functional resolution (61×73×61 voxels, 3 mm isotropic). Defines the 19,629 voxels used in the encoding models and maps results back to volume space.
- **Reference functional NIfTI:** any single preprocessed functional volume in the same space, saved as `ref_func.nii`. Used as a resampling target for the Julich atlas masks. The original analyses used MNI transformed subject 1's run 1 nifti file from the StudyForrest dataset.

### Weights and gaze distributions — `derivatives/weights_and_gaze/`

Two precomputed `.pkl` files are required by `analysis.py` for Figures 3a–d. These were originally exported from a Colab notebook and are not regenerated by the encoding model scripts:

- `all_subs_weights_over_space.pkl` — per-subject baseline model spatial weight maps, shape `(7, 16, 19629)` per subject
- `all_subs_gaze_dists.pkl` — per-subject gaze density heatmaps over the 7×16 spatial grid

Place both files in `derivatives/weights_and_gaze/` (configured via `WEIGHTS_DIR` in `config.py`).

### Participants

The original StudyForrest movie-watching dataset includes 15 participants. 2 were excluded based on eye-tracking data quality, leaving 13 for all analyses:

`sub-01, sub-02, sub-03, sub-04, sub-09, sub-10, sub-14, sub-15, sub-16, sub-17, sub-18, sub-19, sub-20`

**Exclusion criterion.** Participants were excluded based on fixation count per run, normalised within each run across subjects. Outliers were identified using the interquartile range (IQR) method. Rather than excluding individual runs, participants were either included fully or excluded entirely, with any run-level variation in fMRI data length handled by trimming to the available fixation count after feature extraction.

---

## Pipeline

Scripts must be run in the order below. All scripts import paths from `config.py`.

```
run_remodnav.sh                    (Step 0 — skip if using pre-classified data)
         ↓
remodnav_frame_selection.py
         ↓
frames_to_vgg19features.py
         ↓
remodnav_save_features_by_subs.py
         ↓
form_and_save_hyperlayers.py
         ↓
precision_*.py  (encoding models)
```

---

### Step 0 — Run Remodnav on raw eye-tracking data
**Script:** `run_remodnav.sh`

The pipeline takes Remodnav-classified `.tsv` files as input. If you are starting from the raw StudyForrest eye-tracking recordings (`.tsv` physio files from OpenNeuro), you first need to run [Remodnav](https://github.com/psychoinformatics-de/remodnav) to classify gaze events into fixations, saccades, and pursuits.

Install Remodnav (`pip install remodnav`) then run:

```bash
bash run_remodnav.sh
```

This processes all subjects and runs and writes classified event files to `raw/remodnav/`. The key parameters are:

- **Pixel pitch:** `0.03299221912188374` — derived from the StudyForrest display setup (screen size / resolution in the relevant dimension)
- **Sampling rate:** `1000` Hz — the recording rate of the EyeLink eye-tracker used in the StudyForrest dataset

If you are using the pre-classified Remodnav output files already released with the StudyForrest dataset, you can skip this step and place those files directly in `raw/remodnav/`.

---

### Step 1 — Eye-tracking preprocessing
**Script:** `remodnav_frame_selection.py`

Processes Remodnav `.tsv` files to extract fixation events and match them to movie frames. For each fixation event the mean (x, y) gaze coordinate is computed. The movie runs at 25 fps, so frame IDs are derived as `int(onset_in_seconds * 25) + 1`.

**Frame pool design.** Rather than extracting VGG features separately per subject, the script first computes the union of all frames fixated by any subject within each run. Step 2 then extracts features for this shared pool once. Step 3 selects the subject-specific subset from the pool. This avoids redundant CNN forward passes since subjects share a large proportion of fixated frames.

**Out-of-frame fixations.** A small proportion of fixations fall outside the movie frame boundary, most commonly in the vertical axis due to the letterbox format of the stimulus. Sub-09 has the highest number of out-of-frame fixations but also the smallest average deviation from the frame edge. All fixations are retained rather than clipped or excluded, as the out-of-frame events are sufficiently rare and close to the boundary that they do not materially affect the feature sampling.

**Inputs** (`raw/remodnav/`)**:**
- `{sub}-run{run}.tsv`

**Outputs** (`derivatives/fixations/`)**:**
- `{sub}-run{run}.npy` — fixation events with timestamps and (x, y) gaze coordinates, per subject per run
- `Needed_frames_Run_{run}.npy` — union of all frames needed across subjects for each run

---

### Step 2 — CNN feature extraction
**Script:** `frames_to_vgg19features.py`

Passes selected movie frames through VGG-19 (pretrained on ImageNet, with batch normalisation) and extracts activations at 5 max-pooling layers. Feature maps are spatially downsampled to (7, 16) to match the cinematic aspect ratio and to allow cross-layer concatenation downstream. Runs in parallel across the 8 movie runs.

**Inputs:**
- `raw/frames/Run{run}/frame{N}.jpg`
- `derivatives/fixations/Needed_frames_Run_{run}.npy`

**Outputs** (`derivatives/vgg_features/by_run/`)**:**
- `Run{run}_Layer{0-4}_vgg19_features_resized_to_7_16.npy` — shape `(N_frames, C, 7, 16)`

The 5 layers correspond to VGG-19 max-pooling layers following feature blocks of 64, 128, 256, 512, and 512 channels respectively.

---

### Step 3 — Aggregate features by subject
**Script:** `remodnav_save_features_by_subs.py`

Converts run-level feature arrays into subject-specific arrays by selecting only the frames each participant fixated on and concatenating across all 8 runs. Runs in parallel across subjects.

**Inputs:**
- `derivatives/vgg_features/by_run/Run{run}_Layer{0-4}_vgg19_features_resized_to_7_16.npy`
- `derivatives/fixations/Needed_frames_Run_{run}.npy`
- `derivatives/fixations/Needed_frames_{sub}-run{run}.npy`

**Outputs** (`derivatives/vgg_features/by_sub/`)**:**
- `Model_Features_{sub}_concat_run_layer{0-4}_7_16.npy` — shape `(N_fixations, C, 7, 16)`

---

### Step 4 — Form hyperlayer features
**Script:** `form_and_save_hyperlayers.py`

Concatenates the 5 per-layer feature arrays for each subject along the channel dimension into a single hyperlayer feature map (1472 = 64+128+256+512+512 channels). This allows all CNN layers to be used in a single encoding model.

**Inputs:**
- `derivatives/vgg_features/by_sub/Model_Features_{sub}_concat_run_layer{0-4}_7_16.npy`

**Outputs** (`derivatives/vgg_features/hyperlayers/`)**:**
- `Model_features_{sub}_hyperlayer_7_16.npy` — shape `(N_fixations, 1472, 7, 16)`

---

### Step 5 — Train encoding models
**Scripts:** `precision_baseline.py`, `precision_gaze.py`, `precision_centergaze_baseline.py`, `precision_PCA_baseline.py`, `precision_gaze_and_prf.py`

All five scripts share the same fMRI inputs and train/test splitting logic. The fMRI dataset has 3,599 volumes total. The middle 20% of each run is held out as the test set (655 volumes), and a further 20% of the remaining data is used as a validation set for regularisation selection. 4 TRs are removed from the boundaries of each split to prevent autocorrelation leakage. All models use ridge regression; the regularisation parameter is selected by grid search over 10 log-spaced values from 0.1 to 1×10⁸. Performance is evaluated as Pearson correlation on the test set.

The five scripts differ only in how the feature timeseries is constructed from the hyperlayer:

| Script | Feature construction | # features | Output |
|---|---|---|---|
| `precision_baseline.py` | Full spatial map flattened (7×16×1472) | 164,864 | `{sub}_hyperlayer_fast_baseline_results.npy` |
| `precision_gaze.py` | Hyperlayer sampled at subject's actual gaze (x, y) | 1,472 | `{sub}_noPRF_hyperlayer_fast_baseline_results.npy` |
| `precision_centergaze_baseline.py` | Hyperlayer sampled at screen centre for all timepoints | 1,472 | `{sub}_centergaze_hyperlayer_fast_baseline_results.npy` |
| `precision_PCA_baseline.py` | First 1,472 principal components of full feature map | 1,472 | `{sub}_PCA_hyperlayer_fast_baseline_results.npy` |
| `precision_gaze_and_prf.py` | Gaze + per-voxel pRF offset; submitted as SLURM array (one job per subject) | 1,472 | `precision_results_dictionary_list_{sub}.pkl` |

`precision_gaze_and_prf.py` additionally requires pRF estimates from `raw/prf/` and is designed to be submitted as a SLURM job array:

```bash
sbatch --array=0-12 precision_gaze_and_prf.py
```

All results are saved to `derivatives/results/`.

---

## Analyses and Figures

**Script:** `analysis.py`

Produces all main and supplementary figures from the paper. Run after completing the full pipeline (Steps 1–5):

```bash
python analysis.py
```

The script loads all encoding model results, applies group-level statistics, and saves figures as `.png` files to `derivatives/figures/`. The figure-to-section mapping is:

| Figure | Section in script | Description |
|---|---|---|
| Fig. 2a | Performance histograms | Distribution of Pearson r across voxels for all 5 models |
| Fig. 2b | Brain maps | Group-average performance maps (FDR corrected, p < 0.05) |
| Fig. 2c | Difference brain maps | Gaze minus Baseline, Gaze minus Center, Center minus Baseline |
| Fig. 2d | Group violin plots | Per-ROI performance distributions with Bonferroni-corrected pairwise tests |
| Fig. 3a | Heatmap plots | Per-subject gaze distributions and best/worst voxel weight maps |
| Fig. 3b | Gaze–weight correlation brain maps | Spatial correlation between model weights and gaze distribution |
| Fig. 3c (top) | Performance vs. gaze–weight correlation | Subject-level scatter, Gaze and Baseline models |
| Fig. 3c (bottom) | Fixations vs. performance | Normalised fixation count against model performance per subject |
| Fig. 3d | Shannon entropy brain map | Group-average z-scored entropy of baseline model spatial weights |
| Supp. Figs. 1–3 | Single subject brain maps | Individual performance and difference maps for all 13 subjects |
| Supp. Fig. 5 | Single subject violin plots | Per-ROI performance distributions per subject |
| Supp. Fig. 6 | pRF violin plots | Group violins including the PRF+Gaze model |

**Additional inputs required** (beyond encoding model outputs):
- Atlas files in `raw/atlas/` — see [Atlas files](#atlas-files----rawatlas) above
- Precomputed weights and gaze pkl files in `derivatives/weights_and_gaze/` — see [Weights and gaze distributions](#weights-and-gaze-distributions----derivativesweights_and_gaze) above

---

## Dependencies

**Pipeline (Steps 1–5):**
```
numpy scipy scikit-learn torch torchvision joblib tqdm pandas
```

**Analysis and figures (`analysis.py`):**
```
numpy scipy scikit-learn nibabel nilearn siibra seaborn matplotlib pandas
```

Install all at once with:

```bash
pip install numpy scipy scikit-learn torch torchvision joblib tqdm pandas matplotlib nibabel nilearn siibra seaborn
```

---

## Citation

If you use this code, please cite:

```
Gözükara, D., Ahmad, N., Seeliger, K., Oetringer, D., & Geerligs, L. (2026).
Neural network-based encoding in free-viewing fMRI with gaze-aware models. bioRxiv.
```
